# Customer Churn Prediction - Production ML Pipeline

## üìÅ Project Structure

```
Atelier2/
‚îú‚îÄ‚îÄ config/                                     # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml                            # Default configuration
‚îÇ   ‚îú‚îÄ‚îÄ config.dev.yaml                        # Development config
‚îÇ   ‚îî‚îÄ‚îÄ config.prod.yaml                       # Production config
‚îú‚îÄ‚îÄ data/                                       # Datasets directory
‚îÇ   ‚îú‚îÄ‚îÄ churn-bigml-80.csv                     # Training dataset (2,668 records)
‚îÇ   ‚îú‚îÄ‚îÄ churn-bigml-20.csv                     # Test dataset (669 records)
‚îÇ   ‚îî‚îÄ‚îÄ README.md                              # Data documentation
‚îú‚îÄ‚îÄ models/                                     # Trained models directory
‚îÇ   ‚îî‚îÄ‚îÄ v1.0/                                  # Model version 1.0
‚îú‚îÄ‚îÄ config_loader.py                           # Configuration management module
‚îú‚îÄ‚îÄ model_pipeline.py                          # Core ML functions (1,056 lines)
‚îú‚îÄ‚îÄ main.py                                    # Pipeline orchestration (426 lines)
‚îú‚îÄ‚îÄ Makefile                                   # Build automation
‚îú‚îÄ‚îÄ requirements.txt                           # Python dependencies
‚îú‚îÄ‚îÄ CONFIGURATION.md                           # Configuration guide
‚îú‚îÄ‚îÄ ML_Pipeline_Function_Specifications.md     # Detailed function specs
‚îî‚îÄ‚îÄ README.md                                  # This file
```

## üìù Project Overview

**Atelier 2** contains a complete, production-ready machine learning pipeline for customer churn prediction in telecommunications. The project features a **configuration-driven architecture** that eliminates hardcoded values and makes the pipeline truly reusable.

### Key Features
- ‚úÖ **Configuration-Driven** - No hardcoded dataset paths (NEW!)
- ‚úÖ **23 modular functions** for the complete ML lifecycle
- ‚úÖ **Environment-specific configs** (dev, prod)
- ‚úÖ **Hyperparameter optimization** with Optuna (50 trials)
- ‚úÖ **Class balancing** using SMOTEENN
- ‚úÖ **Advanced outlier detection** (Anderson-Darling + Z-score/IQR)
- ‚úÖ **Feature engineering** (Total calls, Total charge, CS call rate)
- ‚úÖ **Model versioning** with metadata tracking
- ‚úÖ **CLI interface** with configuration overrides
- ‚úÖ **MLOps best practices** - Reusable, scalable, production-ready

## üöÄ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Run Full Pipeline

```bash
# Run with default configuration
python main.py

# Use development config (fast, no optimization)
python main.py --config config/config.dev.yaml

# Use production config (full optimization)
python main.py --config config/config.prod.yaml

# Override specific settings
python main.py --config config/config.yaml --model random_forest --no-optimize
```

### 3. Using Makefile

```bash
# Default pipeline
make train

# Development mode (fast)
make train-dev

# Production mode (optimized)
make train-prod

# Only prepare data
make data

# See all available commands
make help
```

### 4. Individual Steps

```bash
# Only prepare data
python main.py --prepare

# Only evaluate saved model
python main.py --evaluate

# Run inference demo
python main.py --predict
```

## üîß Configuration System

The pipeline uses YAML configuration files for maximum flexibility:

### Configuration Files
- **`config/config.yaml`** - Default settings (50 trials, standard optimization)
- **`config/config.dev.yaml`** - Development (fast iteration, no optimization)
- **`config/config.prod.yaml`** - Production (100 trials, full validation)

### Example Configuration
```yaml
data:
  train_path: "data/churn-bigml-80.csv"
  test_path: "data/churn-bigml-20.csv"
  target_column: "Churn"

model:
  type: "xgboost"
  optimization:
    enabled: true
    n_trials: 50
```

### CLI Overrides
```bash
# Override data paths
python main.py --train-data data/new_train.csv --test-data data/new_test.csv

# Override model type
python main.py --config config/config.yaml --model random_forest

# Disable optimization
python main.py --no-optimize
```

**For detailed configuration documentation, see [`CONFIGURATION.md`](CONFIGURATION.md)**

## üì¶ Requirements

```
pandas>=1.3.0
numpy>=1.21.0
scikit-learn>=1.0.0
xgboost>=1.5.0
imbalanced-learn>=0.8.0
scipy>=1.7.0
optuna>=2.10.0
pyyaml>=6.0
```

## üîß Module Functions

### `model_pipeline.py` - Core Functions

#### Data Loading & Validation
- `load_data()` - Load training and test CSV files
- `validate_data_quality()` - Check data quality metrics

#### Preprocessing
- `encode_categorical_features()` - Binary and one-hot encoding
- `detect_normality()` - Anderson-Darling normality test
- `remove_outliers_zscore()` - Remove outliers (normal distributions)
- `remove_outliers_iqr()` - Remove outliers (non-normal distributions)

#### Feature Engineering
- `engineer_features()` - Create derived features
- `select_features_correlation()` - Remove correlated features

#### Data Preparation
- `prepare_data_for_modeling()` - Split features and target
- `balance_dataset()` - SMOTEENN resampling
- `scale_features()` - StandardScaler normalization

#### Model Training
- `train_model()` - Train classification model
- `optimize_hyperparameters()` - Optuna hyperparameter tuning

#### Evaluation
- `evaluate_model()` - Comprehensive metrics
- `cross_validate_model()` - K-fold cross-validation
- `extract_feature_importance()` - Feature ranking

#### Persistence
- `save_model()` - Save model and artifacts
- `load_model()` - Load saved model

#### Inference
- `predict_churn()` - Make predictions on new data
- `prepare_data()` - Complete preprocessing pipeline

## üí° Usage Examples

### Full Pipeline in Code

```python
from model_pipeline import prepare_data, train_model, evaluate_model, save_model

# Prepare data
X_train, y_train, X_test, y_test, artifacts, features = prepare_data()

# Train model
model = train_model(X_train, y_train, model_type='xgboost')

# Evaluate
metrics = evaluate_model(model, X_test, y_test)

# Save
save_model(model, artifacts['scaler'], artifacts['encoders'], 
           features, metadata={'metrics': metrics})
```

### Inference on New Data

```python
from model_pipeline import load_model, predict_churn

# Load saved model
artifacts = load_model(model_dir='./models', version='v1.0')

# New customer data
customer = {
    'State': 'CA',
    'Account length': 100,
    'Area code': 415,
    'International plan': 'No',
    'Voice mail plan': 'Yes',
    # ... other features
}

# Predict
results = predict_churn(customer, artifacts)
print(f"Churn probability: {results['probabilities'][0]:.2%}")
```

### Custom Training

```python
from model_pipeline import prepare_data, optimize_hyperparameters

# Prepare data
X_train, y_train, X_test, y_test, artifacts, features = prepare_data()

# Optimize with 100 trials
model, best_params = optimize_hyperparameters(
    model_type='xgboost',
    X_train=X_train,
    y_train=y_train,
    X_test=X_test,
    y_test=y_test,
    n_trials=100
)

print(f"Best params: {best_params}")
```

## üéØ Key Features

### Configuration System (NEW!)
- ‚úÖ **YAML-based configuration** - No hardcoded values
- ‚úÖ **Environment-specific configs** - Dev, prod configurations
- ‚úÖ **CLI overrides** - Override any config value from command line
- ‚úÖ **Path resolution** - Automatic absolute path handling
- ‚úÖ **Validation** - Configuration validation on load
- ‚úÖ **Reusable** - Same code works with any dataset

### Data Pipeline
- ‚úÖ Automatic data validation
- ‚úÖ Missing value detection
- ‚úÖ Duplicate removal
- ‚úÖ Statistical outlier detection (Anderson-Darling test)
- ‚úÖ Z-score and IQR outlier removal

### Feature Engineering
- ‚úÖ Binary encoding (Yes/No ‚Üí 0/1)
- ‚úÖ One-hot encoding (State, Area code)
- ‚úÖ Derived features (Total calls, Total charge, CS call rate)
- ‚úÖ Correlation-based feature selection
- ‚úÖ StandardScaler normalization

### Model Training
- ‚úÖ Multiple algorithms (XGBoost, Random Forest, Gradient Boosting)
- ‚úÖ SMOTEENN class balancing
- ‚úÖ Optuna hyperparameter optimization
- ‚úÖ 5-fold cross-validation

### Evaluation
- ‚úÖ ROC AUC, Accuracy, Log Loss
- ‚úÖ Cohen's Kappa, Matthews Correlation
- ‚úÖ Confusion Matrix
- ‚úÖ Classification Report
- ‚úÖ Feature Importance Analysis

### Production-Ready
- ‚úÖ **Configuration-driven architecture** (NEW!)
- ‚úÖ Model versioning
- ‚úÖ Artifact persistence (pickle)
- ‚úÖ Metadata tracking (JSON)
- ‚úÖ Error handling
- ‚úÖ Deterministic outputs (RANDOM_STATE=42)
- ‚úÖ Google-style docstrings
- ‚úÖ PEP8 compliant
- ‚úÖ MLOps best practices

## üìä Expected Results

### Model Performance (XGBoost with Optimization)
- **ROC AUC**: ~0.93
- **Accuracy**: ~0.95
- **CV ROC AUC**: ~0.92 ¬± 0.02

### Top Features
1. Total charge
2. Customer service calls
3. International plan
4. Total day charge
5. Total eve charge

## üîí Best Practices

### For Development
```bash
# Use fast mode for iteration
python main.py --no-optimize --model random_forest

# Quick evaluation
python main.py --evaluate
```

### For Production
```bash
# Full optimization
python main.py --optimize --trials 100 --model xgboost

# Version management
# Models saved with timestamps in metadata_v1.0.json
```

### For Deployment
```python
# Always use load_model() to ensure consistent preprocessing
artifacts = load_model(model_dir='./models', version='v1.0')
results = predict_churn(new_data, artifacts)
```

## üêõ Troubleshooting

### Issue: Configuration file not found
```bash
# Ensure config directory exists
ls config/

# Use default config
python main.py --config config/config.yaml
```

### Issue: Data files not found
```bash
# Ensure data files are in data/ directory
ls data/

# Or specify custom paths
python main.py --train-data path/to/train.csv --test-data path/to/test.csv
```

### Issue: Import errors
```bash
# Reinstall all dependencies including PyYAML
pip install -r requirements.txt --upgrade
```

### Issue: Memory errors
```bash
# Use development config (reduced complexity)
python main.py --config config/config.dev.yaml

# Or reduce optimization trials
python main.py --trials 20
```

## üÜï What's New in This Version

### Configuration System Refactor (MLOps Best Practice)
- **Problem Solved**: Eliminated all hardcoded dataset paths that made code non-reusable
- **Solution**: YAML-based configuration system with environment-specific configs
- **Benefits**:
  - ‚úÖ Code is now truly reusable with any dataset
  - ‚úÖ Easy switching between dev/prod environments
  - ‚úÖ Configuration versioning alongside code
  - ‚úÖ CLI overrides for flexibility
  - ‚úÖ Follows industry MLOps standards

### File Organization
- **New**: `config/` directory with YAML configuration files
- **New**: `data/` directory for organized dataset storage
- **New**: `config_loader.py` module for configuration management
- **New**: `CONFIGURATION.md` comprehensive configuration guide
- **Updated**: All functions now accept explicit parameters (no defaults)
- **Updated**: `main.py` now configuration-driven
- **Updated**: `Makefile` with config-aware targets

## üöÄ GitHub Preparation

### Project Status: ‚úÖ Ready for GitHub

All files are organized and production-ready:
- ‚úÖ Clean Python modules (`model_pipeline.py`, `main.py`)
- ‚úÖ Complete documentation (`README.md`, `ML_Pipeline_Function_Specifications.md`)
- ‚úÖ Dependencies listed (`requirements.txt`)
- ‚úÖ `.gitignore` configured
- ‚úÖ Data files included (ensure license allows data sharing)
- ‚úÖ Structured notebook for reference

### Pre-Push Checklist

```bash
# 1. Navigate to Atelier2
cd "c:\Users\Fedih\Downloads\Projet ML\Atelier2"

# 2. Initialize git repository (if not done)
git init

# 3. Add all files
git add .

# 4. Check what will be committed
git status

# 5. Create first commit
git commit -m "feat: Add Atelier2 - Production ML pipeline for churn prediction

- Complete modular pipeline with 23 functions
- Hyperparameter optimization with Optuna
- CLI interface with argparse
- Comprehensive documentation and specifications
- PEP8 compliant, type-hinted code"

# 6. Create GitHub repository and push
git remote add origin https://github.com/YOUR_USERNAME/REPO_NAME.git
git branch -M main
git push -u origin main
```

### Recommended Repository Name
- `churn-prediction-pipeline`
- `ml-churn-atelier2`
- `telecom-churn-ml`

### Suggested GitHub Repository Description
> Production-ready ML pipeline for customer churn prediction in telecommunications. Features modular Python functions, Optuna hyperparameter optimization, SMOTEENN balancing, and comprehensive evaluation (ROC AUC ~0.93). Built with XGBoost, scikit-learn, and FastAPI-ready architecture.

### Suggested Topics/Tags
`machine-learning` `churn-prediction` `xgboost` `scikit-learn` `optuna` `telecommunications` `python` `data-science` `mlops` `production-ml`

## üìä Expected Results

### Model Performance (XGBoost with Optimization)
- **ROC AUC**: ~0.93
- **Accuracy**: ~0.95
- **CV ROC AUC**: ~0.92 ¬± 0.02
- **Training time**: ~2-5 minutes (with 50 trials)

### Top Features
1. Total charge
2. Customer service calls
3. International plan
4. Total day charge
5. Total eve charge

## üìù License

MIT License

## üë• Author

**Fedi Hmida**  
Data Scientist & ML Engineer  
November 2025

---

**Note**: This project is part of a machine learning workshop series (Atelier 2) focusing on transforming exploratory notebooks into production-ready pipelines. 